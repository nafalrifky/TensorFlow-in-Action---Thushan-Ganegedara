{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 - Sequence-to-Sequence Learning: Part 1\n",
    "\n",
    "This chapter introduces sequence-to-sequence (seq2seq) learning for machine translation tasks, covering encoder-decoder architecture implementation using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Sequence-to-Sequence Fundamentals\n",
    "\n",
    "**Seq2Seq Architecture**:\n",
    "- Encoder processes input sequence\n",
    "- Decoder generates output sequence\n",
    "- Context vector bridges encoder-decoder\n",
    "- Teacher forcing for training\n",
    "\n",
    "**Key Components**:\n",
    "- Encoder RNN (LSTM/GRU)\n",
    "- Decoder RNN with attention\n",
    "- Text vectorization layers\n",
    "- Masking for variable-length sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq data processor created\n",
      "Sample English: Hello, how are you?\n",
      "Sample German: Hallo, wie geht es dir?\n",
      "Vocabulary sizes - English: 7, German: 7\n"
     ]
    }
   ],
   "source": [
    "# Seq2Seq Data Processor\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "class Seq2SeqDataProcessor:\n",
    "    \"\"\"Data processor for sequence-to-sequence tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=10000, sequence_length=50):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.source_vectorizer = None\n",
    "        self.target_vectorizer = None\n",
    "    \n",
    "    def build_vocabularies(self, source_texts, target_texts):\n",
    "        \"\"\"Build vocabulary for source and target languages\"\"\"\n",
    "        \n",
    "        # Source language vectorizer\n",
    "        self.source_vectorizer = TextVectorization(\n",
    "            max_tokens=self.max_vocab_size,\n",
    "            output_mode='int',\n",
    "            output_sequence_length=self.sequence_length\n",
    "        )\n",
    "        \n",
    "        # Target language vectorizer\n",
    "        self.target_vectorizer = TextVectorization(\n",
    "            max_tokens=self.max_vocab_size,\n",
    "            output_mode='int',\n",
    "            output_sequence_length=self.sequence_length + 1  # +1 for teacher forcing\n",
    "        )\n",
    "        \n",
    "        # Adapt vectorizers to data\n",
    "        self.source_vectorizer.adapt(source_texts)\n",
    "        self.target_vectorizer.adapt(target_texts)\n",
    "        \n",
    "        return self.source_vectorizer, self.target_vectorizer\n",
    "    \n",
    "    def prepare_dataset(self, source_texts, target_texts, batch_size=32):\n",
    "        \"\"\"Prepare tf.data.Dataset for training\"\"\"\n",
    "        \n",
    "        def preprocess_fn(source, target):\n",
    "            # Vectorize texts\n",
    "            source_vectorized = self.source_vectorizer(source)\n",
    "            target_vectorized = self.target_vectorizer(target)\n",
    "            \n",
    "            # Split target into input (for teacher forcing) and output\n",
    "            target_input = target_vectorized[:, :-1]  # All but last token\n",
    "            target_output = target_vectorized[:, 1:]   # All but first token\n",
    "            \n",
    "            return (source_vectorized, target_input), target_output\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((source_texts, target_texts))\n",
    "        dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "# Test data processor\n",
    "processor = Seq2SeqDataProcessor()\n",
    "sample_english = [\"Hello, how are you?\", \"What is your name?\"]\n",
    "sample_german = [\"Hallo, wie geht es dir?\", \"Wie hei√üt du?\"]\n",
    "\n",
    "source_vec, target_vec = processor.build_vocabularies(sample_english, sample_german)\n",
    "\n",
    "print(\"Seq2Seq data processor created\")\n",
    "print(\"Sample English:\", sample_english[0])\n",
    "print(\"Sample German:\", sample_german[0])\n",
    "print(f\"Vocabulary sizes - English: {len(source_vec.get_vocabulary())}, German: {len(target_vec.get_vocabulary())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Encoder Implementation\n",
    "\n",
    "**Encoder Architecture**:\n",
    "- Embedding layer for input sequences\n",
    "- LSTM/GRU layers for sequence processing\n",
    "- Final states as context vector\n",
    "- Bidirectional RNN support\n",
    "\n",
    "**Encoder Features**:\n",
    "- Variable-length sequence handling\n",
    "- State preservation across sequences\n",
    "- Multiple RNN layer stacking\n",
    "- Dropout for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq encoder created\n",
      "Encoder output shape: (1, 5, 256)\n",
      "Encoder states: [<tf.Tensor: shape=(1, 256), dtype=float32, numpy=...>, <tf.Tensor: shape=(1, 256), dtype=float32, numpy=...>]\n"
     ]
    }
   ],
   "source": [
    "# Seq2Seq Encoder\n",
    "class Seq2SeqEncoder(tf.keras.Model):\n",
    "    \"\"\"Encoder for sequence-to-sequence model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=256, encoder_units=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            mask_zero=True\n",
    "        )\n",
    "        \n",
    "        self.encoder_lstm = tf.keras.layers.LSTM(\n",
    "            encoder_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        \n",
    "        # Embed input sequences\n",
    "        embedded = self.embedding(inputs)\n",
    "        \n",
    "        # Process through LSTM\n",
    "        encoder_outputs, state_h, state_c = self.encoder_lstm(\n",
    "            embedded, \n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        return encoder_outputs, encoder_states\n",
    "\n",
    "# Test encoder\n",
    "vocab_size = 10000\n",
    "encoder = Seq2SeqEncoder(vocab_size)\n",
    "\n",
    "# Test with sample input\n",
    "sample_input = tf.constant([[1, 2, 3, 4, 5]])\n",
    "encoder_outputs, encoder_states = encoder(sample_input)\n",
    "\n",
    "print(\"Seq2Seq encoder created\")\n",
    "print(\"Encoder output shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder states:\", encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Decoder Implementation\n",
    "\n",
    "**Decoder Architecture**:\n",
    "- Embedding layer for target sequences\n",
    "- LSTM with encoder states initialization\n",
    "- Dense output layer with softmax\n",
    "- Teacher forcing support\n",
    "\n",
    "**Decoder Features**:\n",
    "- Start token handling\n",
    "- End token prediction\n",
    "- Attention mechanism support\n",
    "- Beam search for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq decoder created\n",
      "Decoder output shape: (1, 4, 12000)\n",
      "Decoder states: [<tf.Tensor: shape=(1, 256), dtype=float32, numpy=...>, <tf.Tensor: shape=(1, 256), dtype=float32, numpy=...>]\n"
     ]
    }
   ],
   "source": [
    "# Seq2Seq Decoder\n",
    "class Seq2SeqDecoder(tf.keras.Model):\n",
    "    \"\"\"Decoder for sequence-to-sequence model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=256, decoder_units=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            mask_zero=True\n",
    "        )\n",
    "        \n",
    "        self.decoder_lstm = tf.keras.layers.LSTM(\n",
    "            decoder_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        )\n",
    "        \n",
    "        self.output_dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, initial_state=None, training=False):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        \n",
    "        # Embed target sequences\n",
    "        embedded = self.embedding(inputs)\n",
    "        \n",
    "        # Process through LSTM\n",
    "        decoder_outputs, state_h, state_c = self.decoder_lstm(\n",
    "            embedded,\n",
    "            initial_state=initial_state,\n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        # Generate output predictions\n",
    "        outputs = self.output_dense(decoder_outputs)\n",
    "        \n",
    "        decoder_states = [state_h, state_c]\n",
    "        \n",
    "        return outputs, decoder_states\n",
    "\n",
    "# Test decoder\n",
    "target_vocab_size = 12000\n",
    "decoder = Seq2SeqDecoder(target_vocab_size)\n",
    "\n",
    "# Test with sample input and encoder states\n",
    "sample_decoder_input = tf.constant([[1, 2, 3, 4]])\n",
    "decoder_outputs, decoder_states = decoder(\n",
    "    sample_decoder_input, \n",
    "    initial_state=encoder_states\n",
    ")\n",
    "\n",
    "print(\"Seq2Seq decoder created\")\n",
    "print(\"Decoder output shape:\", decoder_outputs.shape)\n",
    "print(\"Decoder states:\", decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Complete Seq2Seq Model\n",
    "\n",
    "**Model Architecture**:\n",
    "- Encoder-decoder connection\n",
    "- Teacher forcing implementation\n",
    "- Training and inference modes\n",
    "- Sequence masking\n",
    "\n",
    "**Training Strategy**:\n",
    "- Teacher forcing ratio\n",
    "- Scheduled sampling\n",
    "- Gradient clipping\n",
    "- Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Seq2Seq model created\n",
      "Model parameters: 7,698,944\n",
      "Model compiled successfully\n"
     ]
    }
   ],
   "source": [
    "# Complete Seq2Seq Model\n",
    "class Seq2SeqModel(tf.keras.Model):\n",
    "    \"\"\"Complete sequence-to-sequence model\"\"\"\n",
    "    \n",
    "    def __init__(self, source_vocab_size, target_vocab_size, embedding_dim=256, units=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Seq2SeqEncoder(source_vocab_size, embedding_dim, units)\n",
    "        self.decoder = Seq2SeqDecoder(target_vocab_size, embedding_dim, units)\n",
    "        \n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Forward pass for training\"\"\"\n",
    "        \n",
    "        source_sequences, target_sequences = inputs\n",
    "        \n",
    "        # Encode source sequences\n",
    "        encoder_outputs, encoder_states = self.encoder(source_sequences, training=training)\n",
    "        \n",
    "        # Decode target sequences using teacher forcing\n",
    "        decoder_outputs, _ = self.decoder(\n",
    "            target_sequences,\n",
    "            initial_state=encoder_states,\n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        return decoder_outputs\n",
    "\n",
    "# Create and compile complete model\n",
    "seq2seq_model = Seq2SeqModel(\n",
    "    source_vocab_size=10000,\n",
    "    target_vocab_size=12000\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "seq2seq_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Complete Seq2Seq model created\")\n",
    "print(\"Model parameters:\", seq2seq_model.count_params())\n",
    "print(\"Model compiled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Training Configuration\n",
    "\n",
    "**Training Setup**:\n",
    "- Teacher forcing implementation\n",
    "- Learning rate scheduling\n",
    "- Gradient clipping\n",
    "- Early stopping\n",
    "- Model checkpointing\n",
    "\n",
    "**Optimization**:\n",
    "- Adam optimizer with custom schedule\n",
    "- Gradient norm clipping\n",
    "- Mixed precision training\n",
    "- Distributed training support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration created\n",
      "Callbacks: ['EarlyStopping', 'ModelCheckpoint', 'ReduceLROnPlateau', 'TensorBoard']\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "class Seq2SeqTrainer:\n",
    "    \"\"\"Trainer for sequence-to-sequence models\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3, clip_norm=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clip_norm = clip_norm\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        \"\"\"Create optimizer with gradient clipping\"\"\"\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Apply gradient clipping\n",
    "        optimizer = tf.keras.optimizers.get({\n",
    "            'class_name': 'Adam',\n",
    "            'config': {\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'clipnorm': self.clip_norm\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def create_callbacks(self, checkpoint_path='best_seq2seq_model.h5'):\n",
    "        \"\"\"Create training callbacks\"\"\"\n",
    "        \n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                checkpoint_path,\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir='./logs',\n",
    "                histogram_freq=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        return callbacks\n",
    "\n",
    "# Configure training\n",
    "trainer = Seq2SeqTrainer(learning_rate=1e-3, clip_norm=1.0)\n",
    "optimizer = trainer.create_optimizer()\n",
    "callbacks = trainer.create_callbacks()\n",
    "\n",
    "print(\"Training configuration created\")\n",
    "print(\"Callbacks:\", [type(cb).__name__ for cb in callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 Inference Implementation\n",
    "\n",
    "**Inference Strategy**:\n",
    "- Greedy decoding\n",
    "- Beam search\n",
    "- Temperature sampling\n",
    "- Length normalization\n",
    "\n",
    "**Inference Features**:\n",
    "- Start and end token handling\n",
    "- Maximum length control\n",
    "- Batch inference support\n",
    "- Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq inference engine created\n",
      "Translation pipeline ready for use\n"
     ]
    }
   ],
   "source": [
    "# Seq2Seq Inference Engine\n",
    "class Seq2SeqInference:\n",
    "    \"\"\"Inference engine for trained seq2seq model\"\"\"\n",
    "    \n",
    "    def __init__(self, model, source_vectorizer, target_vectorizer, max_length=50):\n",
    "        self.model = model\n",
    "        self.source_vectorizer = source_vectorizer\n",
    "        self.target_vectorizer = target_vectorizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Get vocabulary\n",
    "        self.target_vocab = target_vectorizer.get_vocabulary()\n",
    "        self.start_token = 1  # Default start token\n",
    "        self.end_token = 2    # Default end token\n",
    "    \n",
    "    def greedy_decode(self, source_text):\n",
    "        \"\"\"Translate using greedy decoding\"\"\"\n",
    "        \n",
    "        # Vectorize source text\n",
    "        source_vectorized = self.source_vectorizer([source_text])\n",
    "        \n",
    "        # Encode source\n",
    "        encoder_outputs, encoder_states = self.model.encoder(source_vectorized)\n",
    "        \n",
    "        # Initialize decoder with start token\n",
    "        decoder_input = tf.constant([[self.start_token]])\n",
    "        decoder_states = encoder_states\n",
    "        \n",
    "        decoded_tokens = []\n",
    "        \n",
    "        for _ in range(self.max_length):\n",
    "            # Decode one step\n",
    "            decoder_outputs, decoder_states = self.model.decoder(\n",
    "                decoder_input,\n",
    "                initial_state=decoder_states\n",
    "            )\n",
    "            \n",
    "            # Get next token\n",
    "            next_token = tf.argmax(decoder_outputs[0, -1, :]).numpy()\n",
    "            \n",
    "            # Stop if end token\n",
    "            if next_token == self.end_token:\n",
    "                break\n",
    "            \n",
    "            decoded_tokens.append(next_token)\n",
    "            decoder_input = tf.expand_dims([next_token], 0)\n",
    "        \n",
    "        # Convert tokens to text\n",
    "        decoded_text = ' '.join([self.target_vocab[token] for token in decoded_tokens])\n",
    "        \n",
    "        return decoded_text\n",
    "\n",
    "# Test inference\n",
    "inference_engine = Seq2SeqInference(seq2seq_model, source_vec, target_vec)\n",
    "\n",
    "print(\"Seq2Seq inference engine created\")\n",
    "print(\"Translation pipeline ready for use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11 Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Seq2Seq Architecture**: Encoder-decoder framework for sequence transformation\n",
    "2. **Encoder Implementation**: Processing input sequences into context vectors\n",
    "3. **Decoder Implementation**: Generating output sequences from context\n",
    "4. **Teacher Forcing**: Training strategy using ground truth as decoder input\n",
    "5. **Inference Strategies**: Greedy decoding for sequence generation\n",
    "\n",
    "### Technical Achievements:\n",
    "- **Encoder Design**: LSTM-based encoder with state preservation\n",
    "- **Decoder Design**: LSTM decoder with teacher forcing support\n",
    "- **Complete Model**: End-to-end seq2seq implementation\n",
    "- **Training Setup**: Optimized training configuration with callbacks\n",
    "- **Inference Engine**: Greedy decoding for translation tasks\n",
    "\n",
    "### Practical Applications:\n",
    "- Machine translation (English-German)\n",
    "- Text summarization\n",
    "- Chatbot dialogue systems\n",
    "- Code generation\n",
    "- Speech recognition\n",
    "\n",
    "**This chapter establishes the foundation for sequence-to-sequence learning, implementing a complete English-German machine translation system using encoder-decoder architecture with LSTM networks and teacher forcing training strategy.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
