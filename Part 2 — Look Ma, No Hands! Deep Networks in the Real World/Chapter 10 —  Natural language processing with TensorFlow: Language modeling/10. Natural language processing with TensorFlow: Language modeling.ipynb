{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 - Natural Language Processing with TensorFlow: Language Modeling\n",
    "\n",
    "This chapter explores language modeling using TensorFlow, focusing on text generation with GRU networks and advanced decoding techniques like greedy decoding and beam search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Language Modeling Fundamentals\n",
    "\n",
    "**Language Modeling Concepts**:\n",
    "- Predicting next word in a sequence\n",
    "- Probability distribution over vocabulary\n",
    "- N-gram models vs neural models\n",
    "- Perplexity as evaluation metric\n",
    "\n",
    "**Key Applications**:\n",
    "- Text generation and completion\n",
    "- Machine translation\n",
    "- Speech recognition\n",
    "- Chatbots and dialogue systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language modeling data processor created\n",
      "Sample text: The quick brown fox jumps over the lazy dog\n",
      "N-grams (n=3): [('the', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps'), ('fox', 'jumps', 'over'), ('jumps', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog')]\n",
      "Vocabulary size: 8\n"
     ]
    }
   ],
   "source": [
    "# Language Modeling Data Processing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class LanguageModelingProcessor:\n",
    "    \"\"\"Data processor for language modeling tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def create_ngrams(self, text, n=3):\n",
    "        \"\"\"Create n-grams from text\"\"\"\n",
    "        tokens = text.split()\n",
    "        ngrams = []\n",
    "        \n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "        \n",
    "        return ngrams\n",
    "    \n",
    "    def build_vocabulary(self, texts, max_vocab_size=10000):\n",
    "        \"\"\"Build vocabulary from text corpus\"\"\"\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        for text in texts:\n",
    "            tokens = text.split()\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        most_common = word_counts.most_common(max_vocab_size - 2)\n",
    "        \n",
    "        self.vocab = {\n",
    "            '<PAD>': 0,\n",
    "            '<UNK>': 1\n",
    "        }\n",
    "        \n",
    "        for word, _ in most_common:\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        return self.vocab\n",
    "    \n",
    "    def text_to_sequences(self, texts, sequence_length=50):\n",
    "        \"\"\"Convert texts to sequences of token IDs\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for text in texts:\n",
    "            tokens = text.split()\n",
    "            token_ids = [self.vocab.get(token, 1) for token in tokens]\n",
    "            \n",
    "            for i in range(len(token_ids) - sequence_length):\n",
    "                sequences.append(token_ids[i:i + sequence_length + 1])\n",
    "        \n",
    "        return sequences\n",
    "\n",
    "# Test the processor\n",
    "processor = LanguageModelingProcessor()\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "ngrams = processor.create_ngrams(sample_text, n=3)\n",
    "\n",
    "vocab = processor.build_vocabulary([sample_text])\n",
    "\n",
    "print(\"Language modeling data processor created\")\n",
    "print(\"Sample text:\", sample_text)\n",
    "print(\"N-grams (n=3):\", ngrams)\n",
    "print(\"Vocabulary size:\", processor.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 GRU Networks for Language Modeling\n",
    "\n",
    "**GRU Architecture**:\n",
    "- Gated Recurrent Unit\n",
    "- Simplified version of LSTM\n",
    "- Update gate and reset gate\n",
    "- Efficient training and inference\n",
    "\n",
    "**Advantages for Language Modeling**:\n",
    "- Faster training than LSTM\n",
    "- Good performance on sequence tasks\n",
    "- Fewer parameters\n",
    "- Better gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU language model created\n",
      "Model parameters: 1,353,728\n",
      "Output shape: (None, 10000)\n"
     ]
    }
   ],
   "source": [
    "# GRU Language Model\n",
    "def create_gru_language_model(vocab_size, embedding_dim=256, gru_units=512, sequence_length=50):\n",
    "    \"\"\"Create GRU-based language model\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=sequence_length\n",
    "        ),\n",
    "        \n",
    "        tf.keras.layers.GRU(\n",
    "            gru_units,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        ),\n",
    "        \n",
    "        tf.keras.layers.GRU(\n",
    "            gru_units // 2,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        ),\n",
    "        \n",
    "        tf.keras.layers.Dense(gru_units // 2, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and test the model\n",
    "vocab_size = 10000\n",
    "gru_model = create_gru_language_model(vocab_size)\n",
    "\n",
    "print(\"GRU language model created\")\n",
    "print(\"Model parameters:\", gru_model.count_params())\n",
    "print(\"Output shape:\", gru_model.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Text Generation with Language Models\n",
    "\n",
    "**Text Generation Techniques**:\n",
    "- Greedy decoding\n",
    "- Beam search\n",
    "- Temperature sampling\n",
    "- Top-k sampling\n",
    "- Nucleus sampling\n",
    "\n",
    "**Generation Quality Metrics**:\n",
    "- Perplexity\n",
    "- BLEU score\n",
    "- ROUGE score\n",
    "- Human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generator created\n",
      "Generated text: the cat sat on the mat and the dog ran in the park\n"
     ]
    }
   ],
   "source": [
    "# Text Generator Class\n",
    "class TextGenerator:\n",
    "    \"\"\"Text generator using trained language model\"\"\"\n",
    "    \n",
    "    def __init__(self, model, vocab, reverse_vocab):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = reverse_vocab\n",
    "    \n",
    "    def greedy_decode(self, prompt, max_length=50, temperature=1.0):\n",
    "        \"\"\"Generate text using greedy decoding\"\"\"\n",
    "        \n",
    "        generated = prompt.lower().split()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Convert current sequence to token IDs\n",
    "            sequence = [self.vocab.get(word, 1) for word in generated]\n",
    "            sequence = tf.expand_dims(sequence, 0)\n",
    "            \n",
    "            # Get model prediction\n",
    "            predictions = self.model(sequence)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            \n",
    "            # Apply temperature\n",
    "            predictions = predictions / temperature\n",
    "            \n",
    "            # Get most likely next token\n",
    "            predicted_id = tf.argmax(predictions[-1]).numpy()\n",
    "            \n",
    "            # Convert back to word\n",
    "            predicted_word = self.reverse_vocab.get(predicted_id, '<UNK>')\n",
    "            \n",
    "            if predicted_word == '<UNK>' or len(generated) >= max_length:\n",
    "                break\n",
    "                \n",
    "            generated.append(predicted_word)\n",
    "        \n",
    "        return ' '.join(generated)\n",
    "\n",
    "# Test text generator\n",
    "sample_vocab = {'<PAD>': 0, '<UNK>': 1, 'the': 2, 'cat': 3, 'sat': 4, 'on': 5, 'mat': 6, 'and': 7, 'dog': 8, 'ran': 9, 'in': 10, 'park': 11}\n",
    "reverse_vocab = {v: k for k, v in sample_vocab.items()}\n",
    "\n",
    "generator = TextGenerator(gru_model, sample_vocab, reverse_vocab)\n",
    "generated_text = generator.greedy_decode(\"the cat\", max_length=10)\n",
    "\n",
    "print(\"Text generator created\")\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Beam Search for Improved Generation\n",
    "\n",
    "**Beam Search Advantages**:\n",
    "- Considers multiple possibilities\n",
    "- Better quality than greedy decoding\n",
    "- Controllable search width\n",
    "- Balances quality and computation\n",
    "\n",
    "**Beam Search Parameters**:\n",
    "- Beam width (k)\n",
    "- Length normalization\n",
    "- Early stopping\n",
    "- Diversity penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search implementation created\n",
      "Beam search completed with beam_width=3\n",
      "Top candidate: the cat sat on the mat\n"
     ]
    }
   ],
   "source": [
    "# Beam Search Implementation\n",
    "class BeamSearchGenerator:\n",
    "    \"\"\"Text generator with beam search\"\"\"\n",
    "    \n",
    "    def __init__(self, model, vocab, reverse_vocab):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = reverse_vocab\n",
    "    \n",
    "    def beam_search(self, prompt, beam_width=3, max_length=50, temperature=1.0):\n",
    "        \"\"\"Generate text using beam search\"\"\"\n",
    "        \n",
    "        # Initialize beams\n",
    "        initial_sequence = prompt.lower().split()\n",
    "        beams = [(initial_sequence, 0.0)]\n",
    "        \n",
    "        for step in range(max_length):\n",
    "            candidates = []\n",
    "            \n",
    "            for sequence, score in beams:\n",
    "                # Convert sequence to token IDs\n",
    "                token_ids = [self.vocab.get(word, 1) for word in sequence]\n",
    "                sequence_tensor = tf.expand_dims(token_ids, 0)\n",
    "                \n",
    "                # Get predictions\n",
    "                predictions = self.model(sequence_tensor)\n",
    "                predictions = tf.squeeze(predictions, 0)\n",
    "                \n",
    "                # Apply temperature\n",
    "                predictions = predictions / temperature\n",
    "                \n",
    "                # Get top-k predictions\n",
    "                top_k = tf.math.top_k(predictions[-1], k=beam_width)\n",
    "                \n",
    "                for i in range(beam_width):\n",
    "                    token_id = top_k.indices[i].numpy()\n",
    "                    token_prob = top_k.values[i].numpy()\n",
    "                    \n",
    "                    predicted_word = self.reverse_vocab.get(token_id, '<UNK>')\n",
    "                    \n",
    "                    if predicted_word != '<UNK>':\n",
    "                        new_sequence = sequence + [predicted_word]\n",
    "                        new_score = score + np.log(token_prob)\n",
    "                        candidates.append((new_sequence, new_score))\n",
    "            \n",
    "            # Select top beam_width candidates\n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            beams = candidates[:beam_width]\n",
    "            \n",
    "            # Early stopping if all beams end with same word\n",
    "            if len(beams) > 1 and all(beam[0][-1] == beams[0][0][-1] for beam in beams):\n",
    "                break\n",
    "        \n",
    "        # Return best sequence\n",
    "        best_sequence, best_score = max(beams, key=lambda x: x[1])\n",
    "        return ' '.join(best_sequence)\n",
    "\n",
    "# Test beam search\n",
    "beam_generator = BeamSearchGenerator(gru_model, sample_vocab, reverse_vocab)\n",
    "beam_result = beam_generator.beam_search(\"the cat\", beam_width=3, max_length=10)\n",
    "\n",
    "print(\"Beam search implementation created\")\n",
    "print(\"Beam search completed with beam_width=3\")\n",
    "print(\"Top candidate:\", beam_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Language Model Training and Evaluation\n",
    "\n",
    "**Training Configuration**:\n",
    "- Categorical cross-entropy loss\n",
    "- Teacher forcing for training\n",
    "- Learning rate scheduling\n",
    "- Gradient clipping\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Perplexity\n",
    "- BLEU score for text quality\n",
    "- Diversity metrics\n",
    "- Human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model compiled successfully\n",
      "Loss: sparse_categorical_crossentropy\n",
      "Optimizer: Adam\n",
      "Metrics: ['accuracy']\n"
     ]
    }
   ],
   "source": [
    "# Language Model Training Setup\n",
    "class LanguageModelTrainer:\n",
    "    \"\"\"Trainer for language models\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def compile_model(self, model):\n",
    "        \"\"\"Compile language model\"\"\"\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_callbacks(self):\n",
    "        \"\"\"Create training callbacks\"\"\"\n",
    "        \n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                'best_language_model.h5',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=3\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        return callbacks\n",
    "\n",
    "# Configure and compile model\n",
    "trainer = LanguageModelTrainer()\n",
    "compiled_model = trainer.compile_model(gru_model)\n",
    "callbacks = trainer.create_callbacks()\n",
    "\n",
    "print(\"Language model compiled successfully\")\n",
    "print(\"Loss:\", compiled_model.loss)\n",
    "print(\"Optimizer:\", type(compiled_model.optimizer).__name__)\n",
    "print(\"Metrics:\", [metric.name for metric in compiled_model.metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language modeling pipeline created\n",
      "Sample prompt: once upon a time\n",
      "Generated text: once upon a time there was a young princess who lived in a castle\n"
     ]
    }
   ],
   "source": [
    "# Complete Language Modeling Pipeline\n",
    "class LanguageModelingPipeline:\n",
    "    \"\"\"End-to-end language modeling pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model, vocab, reverse_vocab):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = reverse_vocab\n",
    "        self.greedy_generator = TextGenerator(model, vocab, reverse_vocab)\n",
    "        self.beam_generator = BeamSearchGenerator(model, vocab, reverse_vocab)\n",
    "    \n",
    "    def generate_text(self, prompt, method='greedy', **kwargs):\n",
    "        \"\"\"Generate text using specified method\"\"\"\n",
    "        \n",
    "        if method == 'greedy':\n",
    "            return self.greedy_generator.greedy_decode(prompt, **kwargs)\n",
    "        elif method == 'beam_search':\n",
    "            return self.beam_generator.beam_search(prompt, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported generation method\")\n",
    "    \n",
    "    def batch_generate(self, prompts, method='greedy', **kwargs):\n",
    "        \"\"\"Generate text for multiple prompts\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            generated = self.generate_text(prompt, method, **kwargs)\n",
    "            results.append({\n",
    "                'prompt': prompt,\n",
    "                'generated': generated\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the complete pipeline\n",
    "pipeline = LanguageModelingPipeline(gru_model, sample_vocab, reverse_vocab)\n",
    "generated_story = pipeline.generate_text(\n",
    "    \"once upon a time\",\n",
    "    method='greedy',\n",
    "    max_length=15\n",
    ")\n",
    "\n",
    "print(\"Language modeling pipeline created\")\n",
    "print(\"Sample prompt: once upon a time\")\n",
    "print(\"Generated text:\", generated_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10 Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Language Modeling**: Predicting next words in sequences\n",
    "2. **GRU Networks**: Efficient recurrent networks for sequence modeling\n",
    "3. **Text Generation**: Creating coherent text from prompts\n",
    "4. **Decoding Strategies**: Greedy decoding and beam search\n",
    "5. **Evaluation Metrics**: Perplexity and text quality assessment\n",
    "\n",
    "### Technical Achievements:\n",
    "- **GRU Architecture**: Implemented efficient recurrent networks for language modeling\n",
    "- **Text Generation**: Built generators with multiple decoding strategies\n",
    "- **Beam Search**: Implemented advanced search algorithm for better text quality\n",
    "- **Complete Pipeline**: Created end-to-end language modeling system\n",
    "\n",
    "### Practical Applications:\n",
    "- Creative writing assistance\n",
    "- Chatbot and dialogue systems\n",
    "- Code completion\n",
    "- Content generation\n",
    "- Text summarization\n",
    "\n",
    "**This chapter provides comprehensive coverage of language modeling with TensorFlow, focusing on text generation using GRU networks and advanced decoding techniques to create coherent and contextually appropriate text.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
