{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 \u2014 Teaching Machines to See Better: Improving CNNs and Making Them Confess\n",
    "\n",
    "This chapter focuses on advanced techniques to improve CNN performance, including regularization methods, model optimization, transfer learning, and model interpretability using Grad-CAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Techniques for Reducing Overfitting\n",
    "\n",
    "**Overfitting Challenge**: Deep neural networks tend to memorize training data rather than learning generalizable patterns\n",
    "\n",
    "**Solutions**:\n",
    "- **Data Augmentation**: Artificially increase dataset diversity\n",
    "- **Dropout**: Randomly disable neurons during training\n",
    "- **Early Stopping**: Halt training when validation performance plateaus\n",
    "- **Regularization**: Add constraints to model parameters\n",
    "\n",
    "**Goal**: Improve model generalization to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Image Data Augmentation with Keras\n",
    "\n",
    "**Concept**: Apply random transformations to training images to increase dataset variability\n",
    "\n",
    "**Common Transformations**:\n",
    "- Rotation, flipping, zooming\n",
    "- Brightness and contrast adjustments\n",
    "- Shearing and shifting\n",
    "- Color transformations\n",
    "\n",
    "**Benefits**:\n",
    "- Prevents overfitting\n",
    "- Improves model robustness\n",
    "- No additional data collection needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced data augmentation pipeline created\n",
      "Augmented sample shape: (32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Advanced Data Augmentation Pipeline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_advanced_augmentation():\n",
    "    \"\"\"Create comprehensive data augmentation pipeline\"\"\"\n",
    "    \n",
    "    augmentation = tf.keras.Sequential([\n",
    "        # Geometric transformations\n",
    "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomZoom(0.2),\n",
    "        tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
    "        \n",
    "        # Color transformations\n",
    "        tf.keras.layers.RandomContrast(0.2),\n",
    "        tf.keras.layers.RandomBrightness(0.2),\n",
    "        \n",
    "        # Advanced augmentations\n",
    "        tf.keras.layers.RandomShear(0.2),\n",
    "        tf.keras.layers.GaussianNoise(0.1),\n",
    "    ])\n",
    "    \n",
    "    return augmentation\n",
    "\n",
    "# Test augmentation pipeline\n",
    "augmentation_pipeline = create_advanced_augmentation()\n",
    "\n",
    "# Create sample batch\n",
    "sample_batch = tf.random.normal((32, 224, 224, 3))\n",
    "augmented_batch = augmentation_pipeline(sample_batch)\n",
    "\n",
    "print(\"Advanced data augmentation pipeline created\")\n",
    "print(\"Augmented sample shape:\", augmented_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Dropout: Improving Generalizability\n",
    "\n",
    "**Concept**: Randomly set a fraction of input units to 0 during training\n",
    "\n",
    "**Mechanism**:\n",
    "- Forces network to learn redundant representations\n",
    - Prevents co-adaptation of neurons\n",
    - Acts as model averaging\n",
    "\n",
    "**Implementation**:\n",
    "- Typically applied after dense or convolutional layers\n",
    - Dropout rate: 0.2-0.5 for hidden layers, 0.5 for input layer\n",
    - Disabled during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN with dropout regularization created\n",
      "Model summary:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 112, 112, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " spatial_dropout2d (SpatialD  (None, 112, 112, 32)     0         \n",
      " ropout2D)                                                       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 56, 56, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " spatial_dropout2d_1 (Spatia  (None, 56, 56, 64)       0         \n",
      " lDropout2D)                                                     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 200704)            0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200704)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               25690240  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,710,922\n",
      "Trainable params: 25,710,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN with Dropout Regularization\n",
    "def create_cnn_with_dropout(input_shape, num_classes):\n",
    "    \"\"\"Create CNN with comprehensive dropout regularization\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # First convolutional block\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.SpatialDropout2D(0.25),  # Spatial dropout for conv layers\n",
    "        \n",
    "        # Second convolutional block\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.SpatialDropout2D(0.25),\n",
    "        \n",
    "        # Classifier\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),  # High dropout for first dense layer\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Moderate dropout for hidden dense layer\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and display model\n",
    "dropout_model = create_cnn_with_dropout((224, 224, 3), 10)\n",
    "dropout_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"CNN with dropout regularization created\")\n",
    "print(\"Model summary:\")\n",
    "dropout_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 Early Stopping\n",
    "\n",
    "**Concept**: Monitor validation performance and stop training when it stops improving\n",
    "\n",
    "**Implementation**:\n",
    "- Track validation loss or metric\n",
    - Set patience parameter (number of epochs to wait)\n",
    - Restore best weights when stopping\n",
    "\n",
    "**Benefits**:\n",
    - Prevents overfitting\n",
    - Saves computation time\n",
    - Automatic optimal epoch selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced training callbacks configured:\n",
      "- Early stopping with patience=10\n",
      "- Model checkpointing\n",
      "- Learning rate reduction\n",
      "- TensorBoard logging\n"
     ]
    }
   ],
   "source": [
    "# Advanced Training Callbacks\n",
    "def create_advanced_callbacks():\n",
    "    \"\"\"Create comprehensive training callbacks\"\"\"\n",
    "    \n",
    "    callbacks = [\n",
    "        # Early stopping\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Model checkpointing\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_model_weights.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs',\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "# Create callbacks\n",
    "advanced_callbacks = create_advanced_callbacks()\n",
    "print(\"Advanced training callbacks configured:\")\n",
    "print(\"- Early stopping with patience=10\")\n",
    "print(\"- Model checkpointing\")\n",
    "print(\"- Learning rate reduction\")\n",
    "print(\"- TensorBoard logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Minception: Minimalist Inception Architecture\n",
    "\n",
    "**Concept**: Create a simplified yet efficient version of Inception network\n",
    "\n",
    "**Design Principles**:\n",
    "- Reduced computational complexity\n",
    - Maintain multi-scale feature extraction\n",
    - Efficient parameter usage\n",
    - Residual connections for gradient flow\n",
    "\n",
    "**Components**:\n",
    "- Stem: Initial feature extraction\n",
    "- Inception-ResNet blocks: Parallel convolutions with residuals\n",
    "- Reduction blocks: Dimensionality reduction\n",
    - Classifier: Final prediction layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception-ResNet Type A block created\n",
      "Block output shape: (2, 28, 28, 256)\n"
     ]
    }
   ],
   "source": [
    "# Minception Architecture Components\n",
    "def inception_resnet_block_a(x, filters, scale=0.1):\n",
    "    \"\"\"Inception-ResNet Type A block\"\"\"\n",
    "    \n",
    "    # Store input for residual connection\n",
    "    input_tensor = x\n",
    "    \n",
    "    # Branch 1: 1x1 convolution\n",
    "    branch1 = tf.keras.layers.Conv2D(filters[0], (1, 1), activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Branch 2: 1x1 -> 3x3 convolution\n",
    "    branch2 = tf.keras.layers.Conv2D(filters[1], (1, 1), activation='relu', padding='same')(x)\n",
    "    branch2 = tf.keras.layers.Conv2D(filters[2], (3, 3), activation='relu', padding='same')(branch2)\n",
    "    \n",
    "    # Branch 3: 1x1 -> 3x3 -> 3x3 convolution\n",
    "    branch3 = tf.keras.layers.Conv2D(filters[3], (1, 1), activation='relu', padding='same')(x)\n",
    "    branch3 = tf.keras.layers.Conv2D(filters[4], (3, 3), activation='relu', padding='same')(branch3)\n",
    "    branch3 = tf.keras.layers.Conv2D(filters[5], (3, 3), activation='relu', padding='same')(branch3)\n",
    "    \n",
    "    # Concatenate branches\n",
    "    concatenated = tf.keras.layers.concatenate([branch1, branch2, branch3], axis=-1)\n",
    "    \n",
    "    # 1x1 projection to match dimensions\n",
    "    projected = tf.keras.layers.Conv2D(tf.keras.backend.int_shape(input_tensor)[-1], (1, 1), padding='same')(concatenated)\n",
    "    \n",
    "    # Add residual connection with scaling\n",
    "    output = tf.keras.layers.Lambda(lambda x: x[0] + x[1] * scale)([input_tensor, projected])\n",
    "    output = tf.keras.layers.Activation('relu')(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test the block\n",
    "input_tensor = tf.keras.layers.Input(shape=(28, 28, 64))\n",
    "output = inception_resnet_block_a(input_tensor, [32, 32, 64, 32, 64, 64])\n",
    "block_model = tf.keras.Model(inputs=input_tensor, outputs=output)\n",
    "\n",
    "test_input = tf.random.normal((2, 28, 28, 64))\n",
    "test_output = block_model(test_input)\n",
    "\n",
    "print(\"Inception-ResNet Type A block created\")\n",
    "print(\"Block output shape:\", test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Minception model created\n",
      "Model parameters: 1,285,962\n"
     ]
    }
   ],
   "source": [
    "# Complete Minception Model\n",
    "def create_minception_model(input_shape, num_classes):\n",
    "    \"\"\"Create complete Minception model\"\"\"\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Stem\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), strides=(2, 2), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    # Inception-ResNet blocks\n",
    "    x = inception_resnet_block_a(x, [32, 32, 64, 32, 64, 64])\n",
    "    x = inception_resnet_block_a(x, [64, 64, 96, 64, 96, 96])\n",
    "    \n",
    "    # Reduction block\n",
    "    x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    # More Inception-ResNet blocks\n",
    "    x = inception_resnet_block_a(x, [96, 96, 128, 96, 128, 128])\n",
    "    x = inception_resnet_block_a(x, [128, 128, 192, 128, 192, 192])\n",
    "    \n",
    "    # Classifier\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create Minception model\n",
    "minception_model = create_minception_model((224, 224, 3), 10)\n",
    "minception_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Complete Minception model created\")\n",
    "print(\"Model parameters:\", minception_model.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Transfer Learning with Pretrained Networks\n",
    "\n",
    "**Concept**: Leverage knowledge from models trained on large datasets (e.g., ImageNet)\n",
    "\n",
    "**Approaches**:\n",
    "- **Feature Extraction**: Use pretrained model as fixed feature extractor\n",
    "- **Fine-tuning**: Update some layers of pretrained model\n",
    "- **Progressive Unfreezing**: Gradually unfreeze layers during training\n",
    "\n",
    "**Benefits**:\n",
    - Faster training\n",
    - Better performance with small datasets\n",
    - Leverages learned feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer learning model created with EfficientNetB0 backbone\n",
      "Base model layers frozen: 235\n"
     ]
    }
   ],
   "source": [
    "# Advanced Transfer Learning Implementation\n",
    "def create_transfer_learning_model(base_model_name='EfficientNetB0', num_classes=10, fine_tune_layers=10):\n",
    "    \"\"\"Create transfer learning model with flexible backbone\"\"\"\n",
    "    \n",
    "    # Load pretrained base model\n",
    "    if base_model_name == 'EfficientNetB0':\n",
    "        base_model = tf.keras.applications.EfficientNetB0(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = tf.keras.applications.ResNet50(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported base model\")\n",
    "    \n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Create transfer learning model\n",
    "    inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "    \n",
    "    # Preprocessing (model-specific)\n",
    "    if 'EfficientNet' in base_model_name:\n",
    "        x = tf.keras.applications.efficientnet.preprocess_input(inputs)\n",
    "    else:\n",
    "        x = tf.keras.applications.resnet.preprocess_input(inputs)\n",
    "    \n",
    "    # Base model features\n",
    "    x = base_model(x, training=False)\n",
    "    \n",
    "    # Custom classifier\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Function to unfreeze layers for fine-tuning\n",
    "    def unfreeze_for_fine_tuning():\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        # Freeze all layers first\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Unfreeze last N layers\n",
    "        for layer in base_model.layers[-fine_tune_layers:]:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    model.unfreeze_for_fine_tuning = unfreeze_for_fine_tuning\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Create transfer learning model\n",
    "transfer_model, base_model = create_transfer_learning_model()\n",
    "transfer_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Transfer learning model created with EfficientNetB0 backbone\")\n",
    "print(\"Base model layers frozen:\", len([l for l in base_model.layers if not l.trainable]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Grad-CAM: Making CNNs Confess\n",
    "\n",
    "**Concept**: Gradient-weighted Class Activation Mapping - visualize which parts of the image influenced the model's decision\n",
    "\n",
    "**How it works**:\n",
    "- Compute gradients of target class with respect to final convolutional layer\n",
    "- Create heatmap showing important regions\n",
    - Combine with original image for visualization\n",
    "\n",
    "**Applications**:\n",
    - Model interpretability\n",
    - Debugging model decisions\n",
    - Building trust in AI systems\n",
    - Identifying model biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad-CAM implementation created\n",
      "Heatmap shape: (14, 14)\n"
     ]
    }
   ],
   "source": [
    "# Grad-CAM Implementation\n",
    "class GradCAM:\n",
    "    \"\"\"Gradient-weighted Class Activation Mapping\"\"\"\n",
    "    \n",
    "    def __init__(self, model, layer_name):\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name\n",
    "        self.grad_model = tf.keras.models.Model(\n",
    "            inputs=[model.inputs],\n",
    "            outputs=[model.get_layer(layer_name).output, model.output]\n",
    "        )\n",
    "    \n",
    "    def compute_heatmap(self, image, class_idx=None, eps=1e-8):\n",
    "        \"\"\"Compute Grad-CAM heatmap for given image and class\"\"\"\n",
    "        \n",
    "        # Record operations for gradient computation\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, predictions = self.grad_model(image)\n",
    "            \n",
    "            if class_idx is None:\n",
    "                class_idx = tf.argmax(predictions[0])\n",
    "            \n",
    "            loss = predictions[:, class_idx]\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "        \n",
    "        # Global average pooling of gradients\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        \n",
    "        # Weight feature maps by gradients\n",
    "        conv_outputs = conv_outputs[0]\n",
    "        heatmap = tf.reduce_mean(tf.multiply(conv_outputs, pooled_grads), axis=-1)\n",
    "        \n",
    "        # Normalize heatmap\n",
    "        heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + eps)\n",
    "        \n",
    "        return heatmap.numpy()\n",
    "    \n",
    "    def overlay_heatmap(self, heatmap, image, alpha=0.4):\n",
    "        \"\"\"Overlay heatmap on original image\"\"\"\n",
    "        \n",
    "        # Resize heatmap to match image dimensions\n",
    "        heatmap = tf.image.resize(\n",
    "            heatmap[..., tf.newaxis], \n",
    "            [image.shape[0], image.shape[1]]\n",
    "        ).numpy().squeeze()\n",
    "        \n",
    "        # Apply colormap\n",
    "        heatmap_colored = plt.cm.jet(heatmap)[..., :3]\n",
    "        \n",
    "        # Overlay heatmap on image\n",
    "        overlayed = heatmap_colored * alpha + image * (1 - alpha)\n",
    "        \n",
    "        return np.clip(overlayed, 0, 1)\n",
    "\n",
    "# Test Grad-CAM implementation\n",
    "def create_test_model_for_gradcam():\n",
    "    \"\"\"Create a simple model for Grad-CAM testing\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create test setup\n",
    "test_model = create_test_model_for_gradcam()\n",
    "grad_cam = GradCAM(test_model, 'conv2d_1')\n",
    "\n",
    "# Generate test heatmap\n",
    "test_image = tf.random.normal((1, 224, 224, 3))\n",
    "heatmap = grad_cam.compute_heatmap(test_image)\n",
    "\n",
    "print(\"Grad-CAM implementation created\")\n",
    "print(\"Heatmap shape:\", heatmap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete model interpretability pipeline created\n",
      "Available visualization methods: Grad-CAM, Saliency Maps, Feature Visualization\n"
     ]
    }
   ],
   "source": [
    "# Complete Model Interpretability Pipeline\n",
    "class ModelInterpretability:\n",
    "    \"\"\"Comprehensive model interpretability toolkit\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.grad_cam = {}\n",
    "    \n",
    "    def register_layer_for_gradcam(self, layer_name):\n",
    "        \"\"\"Register a layer for Grad-CAM analysis\"\"\"\n",
    "        self.grad_cam[layer_name] = GradCAM(self.model, layer_name)\n",
    "    \n",
    "    def analyze_prediction(self, image, top_k=3):\n",
    "        \"\"\"Comprehensive analysis of model prediction\"\"\"\n",
    "        \n",
    "        # Get prediction\n",
    "        predictions = self.model.predict(image)\n",
    "        top_classes = np.argsort(predictions[0])[-top_k:][::-1]\n",
    "        \n",
    "        analysis = {\n",
    "            'predictions': predictions[0],\n",
    "            'top_classes': top_classes,\n",
    "            'heatmaps': {}\n",
    "        }\n",
    "        \n",
    "        # Generate heatmaps for top classes\n",
    "        for layer_name, grad_cam in self.grad_cam.items():\n",
    "            analysis['heatmaps'][layer_name] = {}\n",
    "            for class_idx in top_classes:\n",
    "                heatmap = grad_cam.compute_heatmap(image, class_idx)\n",
    "                analysis['heatmaps'][layer_name][class_idx] = heatmap\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def visualize_analysis(self, analysis, image, layer_name):\n",
    "        \"\"\"Visualize interpretability analysis\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, len(analysis['top_classes']), figsize=(15, 8))\n",
    "        \n",
    "        for i, class_idx in enumerate(analysis['top_classes']):\n",
    "            # Original image with heatmap\n",
    "            heatmap = analysis['heatmaps'][layer_name][class_idx]\n",
    "            overlayed = self.grad_cam[layer_name].overlay_heatmap(heatmap, image[0])\n",
    "            \n",
    "            axes[0, i].imshow(overlayed)\n",
    "            axes[0, i].set_title(f'Class {class_idx} (p={analysis[\"predictions\"][class_idx]:.3f})')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Heatmap only\n",
    "            axes[1, i].imshow(heatmap, cmap='jet')\n",
    "            axes[1, i].set_title(f'Heatmap - Class {class_idx}')\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.tight
