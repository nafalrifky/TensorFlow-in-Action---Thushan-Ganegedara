{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 — State-of-the-Art in Deep Learning: Transformers\n",
    "This chapter explores Transformer models, which have revolutionized natural language processing and achieved state-of-the-art performance across various tasks. We'll dive into the architecture, self-attention mechanism, and practical implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Representing Text as Numbers\n",
    "Before feeding text into Transformer models, we need to convert words into numerical representations that neural networks can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing and tokenization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "\n",
    "# Sample text data\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Transformers are amazing for NLP tasks\",\n",
    "    \"Attention is all you need for sequence processing\"\n",
    "]\n",
    "\n",
    "# Initialize TextVectorization layer\n",
    "max_tokens = 1000\n",
    "output_sequence_length = 10\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=output_sequence_length\n",
    ")\n",
    "\n",
    "# Adapt the layer to the text data\n",
    "vectorize_layer.adapt(sample_texts)\n",
    "\n",
    "# Transform text to sequences\n",
    "vectorized_texts = vectorize_layer(sample_texts)\n",
    "\n",
    "print(\"Original texts:\")\n",
    "for text in sample_texts:\n",
    "    print(f\"  {text}\")\n",
    "    \n",
    "print(\"\\nVectorized sequences:\")\n",
    "print(vectorized_texts.numpy())\n",
    "\n",
    "# Get vocabulary\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(f\"\\nFirst 10 vocabulary items: {vocab[:10]}\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Word Embeddings\n",
    "Word embeddings convert token indices into dense vector representations that capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word embeddings\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_dim = 64\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=len(vocab),\n",
    "    output_dim=embedding_dim,\n",
    "    input_length=output_sequence_length,\n",
    "    name=\"word_embedding\"\n",
    ")\n",
    "\n",
    "# Apply embeddings to vectorized text\n",
    "embedded_texts = embedding_layer(vectorized_texts)\n",
    "\n",
    "print(f\"Vectorized text shape: {vectorized_texts.shape}\")\n",
    "print(f\"Embedded text shape: {embedded_texts.shape}\")\n",
    "print(f\"Embedding layer weights shape: {embedding_layer.weights[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Understanding the Transformer Model\n",
    "The Transformer architecture introduced in \"Attention Is All You Need\" replaces recurrent layers with self-attention mechanisms for better parallelization and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Self-Attention Mechanism\n",
    "Self-attention allows each position in the sequence to attend to all other positions, capturing global dependencies efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing scaled dot-product attention\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"Calculate the attention weights and output\"\"\"\n",
    "    # Matrix multiplication of query and key\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "    \n",
    "    # Scale by square root of depth\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # Multiply by values\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test the attention mechanism\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "depth = 64\n",
    "\n",
    "query = tf.random.normal((batch_size, seq_length, depth))\n",
    "key = tf.random.normal((batch_size, seq_length, depth))\n",
    "value = tf.random.normal((batch_size, seq_length, depth))\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Key shape: {key.shape}\")\n",
    "print(f\"Value shape: {value.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nSample attention weights for first sequence:\")\n",
    "print(attention_weights[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Multi-Head Attention\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing multi-head attention\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth)\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, query, key, value, mask=None):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Linear transformations\n",
    "        query = self.wq(query)\n",
    "        key = self.wk(key)\n",
    "        value = self.wv(value)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            query, key, value, mask\n",
    "        )\n",
    "        \n",
    "        # Concatenate heads\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                    (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "query = tf.random.normal((batch_size, seq_length, d_model))\n",
    "key = tf.random.normal((batch_size, seq_length, d_model))\n",
    "value = tf.random.normal((batch_size, seq_length, d_model))\n",
    "\n",
    "output, attention_weights = mha(query, key, value)\n",
    "\n",
    "print(f\"Input query shape: {query.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Transformer Encoder Layer\n",
    "The encoder layer consists of multi-head attention followed by position-wise feed-forward networks with residual connections and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Transformer encoder layer\n",
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask=None):\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        # Feed-forward network with residual connection\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "# Test encoder layer\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model, num_heads, dff)\n",
    "\n",
    "sample_input = tf.random.normal((batch_size, seq_length, d_model))\n",
    "output = encoder_layer(sample_input, training=False)\n",
    "\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Positional Encoding\n",
    "Since Transformers don't have recurrence or convolution, we need to add positional information to help the model understand sequence order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing positional encoding\n",
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"Generate positional encoding matrix\"\"\"\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "    \n",
    "    # Apply sine to even indices\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cosine to odd indices\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    \"\"\"Calculate angles for positional encoding\"\"\"\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "# Test positional encoding\n",
    "position = 50\n",
    "d_model = 128\n",
    "\n",
    "pos_encoding = positional_encoding(position, d_model)\n",
    "\n",
    "print(f\"Positional encoding shape: {pos_encoding.shape}\")\n",
    "print(f\"Positional encoding for position 0: {pos_encoding[0, 0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Complete Transformer Model\n",
    "Now let's build a complete Transformer model for a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a complete Transformer model\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 pe_input, pe_target, rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, dff, \n",
    "                                        input_vocab_size, pe_input, rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, training, enc_padding_mask=None):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(enc_output)\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "                 input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                              d_model)\n",
    "        \n",
    "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate) \n",
    "                         for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Add embedding and position encoding\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the complete model\n",
    "num_layers = 2\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "input_vocab_size = 1000\n",
    "target_vocab_size = 1000\n",
    "maximum_position_encoding = 1000\n",
    "\n",
    "transformer = TransformerModel(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    pe_input=maximum_position_encoding,\n",
    "    pe_target=maximum_position_encoding\n",
    ")\n",
    "\n",
    "# Test with sample input\n",
    "sample_input = tf.random.uniform((batch_size, seq_length), \n",
    "                               maxval=input_vocab_size, dtype=tf.int32)\n",
    "output = transformer(sample_input, training=False)\n",
    "\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Practical Applications\n",
    "Let's implement a practical text classification task using Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification with Transformer\n",
    "class TextClassificationTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "                 vocab_size, max_position_encoding, num_classes, rate=0.1):\n",
    "        super(TextClassificationTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_position_encoding, d_model)\n",
    "        \n",
    "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate) \n",
    "                         for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.global_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Add embedding and position encoding\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training)\n",
    "        \n",
    "        # Global average pooling and classification\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create and test the classification model\n",
    "num_classes = 3\n",
    "max_sequence_length = 100\n",
    "\n",
    "classifier = TextClassificationTransformer(\n",
    "    num_layers=2,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    dff=512,\n",
    "    vocab_size=1000,\n",
    "    max_position_encoding=max_sequence_length,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Test with sample input\n",
    "sample_input = tf.random.uniform((batch_size, max_sequence_length), \n",
    "                               maxval=1000, dtype=tf.int32)\n",
    "output = classifier(sample_input, training=False)\n",
    "\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Sample predictions: {output[0].numpy()}\")\n",
    "print(f\"Predicted class: {tf.argmax(output[0]).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Training the Transformer\n",
    "Let's set up the training process for our Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"Create padding mask for sequences\"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# Compile the model\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Transformer classifier compiled successfully!\")\n",
    "print(f\"Total parameters: {classifier.count_params():,}\")\n",
    "\n",
    "# Model summary\n",
    "classifier.build(input_shape=(None, max_sequence_length))\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Transformer Concepts\n",
    "\n",
    "### Table 5.1 — Transformer Components\n",
    "| Component | Purpose | Key Features |\n",
    "|-----------|---------|--------------|\n",
    "| Self-Attention | Capture dependencies between all positions | Scaled dot-product, parallel computation |\n",
    "| Multi-Head Attention | Attend to different representation subspaces | Multiple attention heads, concatenated outputs |\n",
    "| Positional Encoding | Inject sequence order information | Sine/cosine functions, fixed patterns |\n",
    "| Feed-Forward Network | Apply non-linear transformations | Position-wise, two linear layers with ReLU |\n",
    "| Layer Normalization | Stabilize training | Normalize across features, learnable parameters |\n",
    "| Residual Connections | Facilitate gradient flow | Add input to output, prevent vanishing gradients |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Causal Masking\n",
    "Implement causal (look-ahead) masking for decoder self-attention to prevent attending to future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution: Causal Masking\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"Create look-ahead mask for decoder self-attention\"\"\"\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "# Test causal masking\n",
    "seq_len = 5\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "print(\"Look-ahead mask:\")\n",
    "print(look_ahead_mask.numpy())\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"0 = allowed to attend, 1 = masked (not allowed to attend)\")\n",
    "print(\"This ensures each position can only attend to previous positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Learning Rate Scheduling\n",
    "Implement the learning rate schedule used in the original Transformer paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution: Learning Rate Schedule\n",
    "class TransformerLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(TransformerLearningRateSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        \n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        lr = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "        \n",
    "        return lr\n",
    "\n",
    "# Test learning rate schedule\n",
    "d_model = 128\n",
    "lr_schedule = TransformerLearningRateSchedule(d_model)\n",
    "\n",
    "print(\"Learning rate at different steps:\")\n",
    "for step in [1, 1000, 4000, 10000]:\n",
    "    lr = lr_schedule(step)\n",
    "    print(f\"Step {step}: learning rate = {lr:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Beam Search\n",
    "Implement beam search for sequence generation with Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution: Beam Search\n",
    "def beam_search_decode(model, start_token, max_length, beam_width=3):\n",
    "    \"\"\"Simple beam search implementation\"\"\"\n",
    "    # Initialize beams with start token and score 0\n",
    "    beams = [([start_token], 0.0)]\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        all_candidates = []\n",
    "        \n",
    "        # Expand each beam\n",
    "        for sequence, score in beams:\n",
    "            # Get model predictions (simulated)\n",
    "            # In practice, you'd feed the sequence to your model\n",
    "            predictions = tf.random.uniform((1, 1000))\n",
    "            predictions = tf.nn.log_softmax(predictions, axis=-1)\n",
    "            \n",
    "            # Get top k predictions\n",
    "            top_k_scores, top_k_indices = tf.math.top_k(predictions[0], k=beam_width)\n",
    "            \n",
    "            for i in range(beam_width):\n",
    "                candidate_sequence = sequence + [top_k_indices[i].numpy()]\n",
    "                candidate_score = score + top_k_scores[i].numpy()\n",
    "                all_candidates.append((candidate_sequence, candidate_score))\n",
    "        \n",
    "        # Select top beam_width candidates\n",
    "        beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "    \n",
    "    return beams\n",
    "\n",
    "# Test beam search\n",
    "start_token = 1\n",
    "max_length = 5\n",
    "beam_width = 3\n",
    "\n",
    "results = beam_search_decode(None, start_token, max_length, beam_width)\n",
    "\n",
    "print(\"Beam search results:\")\n",
    "for i, (sequence, score) in enumerate(results):\n",
    "    print(f\"Beam {i+1}: Sequence={sequence}, Score={score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 Summary\n",
    "\n",
    "This chapter introduced Transformer models, which have become the state-of-the-art architecture for natural language processing and beyond:\n",
    "\n",
    "1. **Self-Attention Mechanism**:\n",
    "   - Allows each position to attend to all other positions\n",
    "   - Captures global dependencies efficiently\n",
    "   - Enables parallel computation\n",
    "\n",
    "2. **Multi-Head Attention**:\n",
    "   - Attends to different representation subspaces\n",
    "   - Improves model capacity and performance\n",
    "   - Multiple attention heads work in parallel\n",
    "\n",
    "3. **Positional Encoding**:\n",
    "   - Injects sequence order information\n",
    "   - Uses sine and cosine functions\n",
    "   - Fixed patterns that don't require learning\n",
    "\n",
    "4. **Encoder Architecture**:\n",
    "   - Stack of identical layers\n",
    "   - Each layer has multi-head attention and feed-forward network\n",
    "   - Residual connections and layer normalization\n",
    "\n",
    "5. **Practical Applications**:\n",
    "   - Text classification\n",
    "   - Machine translation\n",
    "   - Sequence generation\n",
    "   - Various NLP tasks\n",
    "\n",
    "**Key Advantages of Transformers**:\n",
    "- Superior parallelization compared to RNNs\n",
    - Better handling of long-range dependencies\n",
    "- State-of-the-art performance across NLP tasks\n",
    "- Scalable to very large models (BERT, GPT, T5)\n",
    "\n",
    "**Next Steps**:\n",
    "- Explore pre-trained Transformer models (BERT, GPT, T5)\n",
    "- Learn about different attention variants\n",
    "- Study efficient Transformer architectures\n",
    "- Apply Transformers to different domains (vision, audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4
