{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 — State-of-the-Art in Deep Learning: Transformers\n",
    "\n",
    "This chapter explores the Transformer model, which has revolutionized natural language processing by replacing recurrent and convolutional architectures with self-attention mechanisms. Transformers enable parallel computation and better handling of long-range dependencies in sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Representing Text as Numbers\n",
    "\n",
    "**Concept**: Before processing text with Transformers, we need to convert words into numerical representations that neural networks can understand.\n",
    "\n",
    "**Process**:\n",
    "- **Tokenization**: Split text into individual tokens/words\n",
    "- **Vectorization**: Convert tokens to numerical indices\n",
    "- **Embedding**: Transform indices into dense vectors that capture semantic meaning\n",
    "\n",
    "**Purpose**: Enables neural networks to process and learn from textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokenization and vectorization example\n",
    "import tensorflow as tf\n",
    "\n",
    "texts = [\"Transformers are amazing\", \"Attention is all you need\"]\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=1000, output_sequence_length=10)\n",
    "vectorize_layer.adapt(texts)\n",
    "\n",
    "vectorized = vectorize_layer(texts)\n",
    "print(\"Vectorized texts:\", vectorized.numpy())\n",
    "print(\"Vocabulary size:\", len(vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Self-Attention Mechanism\n",
    "\n",
    "**Concept**: Self-attention allows each position in a sequence to attend to all other positions, capturing global dependencies efficiently.\n",
    "\n",
    "**Components**:\n",
    "- **Query**: What we're looking for\n",
    "- **Key**: What we can offer\n",
    "- **Value**: The actual values to aggregate\n",
    "\n",
    "**Formula**: Attention = Softmax(QKᵀ/√dₖ)V\n",
    "\n",
    "**Advantage**: Efficiently captures long-range dependencies without sequential processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    \"\"\"Implementation of scaled dot-product attention\"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    return tf.matmul(attention_weights, value), attention_weights\n",
    "\n",
    "# Test the attention mechanism\n",
    "query = tf.random.normal((2, 5, 64))\n",
    "output, weights = scaled_dot_product_attention(query, query, query)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Attention weights shape:\", weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Multi-Head Attention\n",
    "\n",
    "**Concept**: Multi-head attention allows the model to jointly attend to information from different representation subspaces.\n",
    "\n",
    "**Mechanism**:\n",
    "- Split embeddings into multiple \"heads\"\n",
    "- Each head learns different attention patterns\n",
    "- Concatenate outputs from all heads\n",
    "\n",
    "**Benefits**:\n",
    "- Increases model capacity\n",
    "- Enables learning of complex patterns\n",
    "- More robust to noise\n",
    "- Captures different types of relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def call(self, query, key, value):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Linear transformations\n",
    "        query = self.wq(query)\n",
    "        key = self.wk(key)\n",
    "        value = self.wv(value)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        query = tf.reshape(query, (batch_size, -1, self.num_heads, self.depth))\n",
    "        query = tf.transpose(query, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # Similar processing for key and value...\n",
    "        \n",
    "        return query  # Simplified for demonstration\n",
    "\n",
    "# Test multi-head attention\n",
    "mha = MultiHeadAttention(d_model=128, num_heads=8)\n",
    "output = mha(tf.random.normal((2, 10, 128)))\n",
    "print(\"Multi-head attention output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Positional Encoding\n",
    "\n",
    "**Problem**: Unlike RNNs, Transformers don't have inherent sequence information, so we need to explicitly add positional information.\n",
    "\n",
    "**Solution**: Positional Encoding\n",
    "- Uses sine and cosine functions\n",
    - Fixed values that don't require learning\n",
    "- Each position has a unique encoding\n",
    "\n",
    "**Formula**:\n",
    "- Even positions: sin(pos/10000^(2i/d_model))\n",
    "- Odd positions: cos(pos/10000^(2i/d_model))\n",
    "\n",
    "**Purpose**: Provides the model with information about token positions in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"Generate positional encoding matrix\"\"\"\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "    \n",
    "    # Apply sine to even indices\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # Apply cosine to odd indices\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "# Test positional encoding\n",
    "pos_encoding = positional_encoding(50, 128)\n",
    "print(\"Positional encoding shape:\", pos_encoding.shape)\n",
    "print(\"Sample values:\", pos_encoding[0, 0, :5].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Complete Transformer Architecture\n",
    "\n",
    "**Core Components**:\n",
    "1. **Encoder Layers**: Stack of identical layers\n",
    "2. **Multi-Head Attention**: Captures dependencies between tokens\n",
    "3. **Feed-Forward Network**: Applies non-linear transformations\n",
    "4. **Residual Connections**: Facilitates gradient flow during training\n",
    "5. **Layer Normalization**: Stabilizes training process\n",
    "\n",
    "**Data Flow**:\n",
    "Input → Embedding + Positional Encoding → [MHA → Add & Norm → FFN → Add & Norm] × N → Output\n",
    "\n",
    "**Key Innovation**: Replaces sequential processing with parallel attention mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training):\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        # Feed-forward network with residual connection\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "# Test encoder layer\n",
    "encoder_layer = TransformerEncoderLayer(d_model=128, num_heads=8, dff=512)\n",
    "sample_input = tf.random.normal((2, 10, 128))\n",
    "output = encoder_layer(sample_input, training=False)\n",
    "print(\"Encoder input shape:\", sample_input.shape)\n",
    "print(\"Encoder output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Practical Application: Text Classification\n",
    "\n",
    "**Use Case**: Using Transformer architecture for text classification tasks like sentiment analysis\n",
    "\n",
    "**Architecture**:\n",
    "- Input: Sequence of token indices\n",
    "- Transformer Encoder: Processes the entire sequence\n",
    "- Global Average Pooling: Aggregates sequence information\n",
    "- Classifier: Produces final probability distribution\n",
    "\n",
    "**Advantages**:\n",
    "- Captures global context effectively\n",
    "- Enables parallel computation\n",
    "- State-of-the-art performance on various NLP tasks\n",
    "- Handles long-range dependencies better than RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_len, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_len, d_model)\n",
    "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.global_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Add embedding and positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Process through encoder layers\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, training)\n",
    "        \n",
    "        # Classification head\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Create and test the model\n",
    "model = TextClassificationTransformer(\n",
    "    num_layers=2, d_model=128, num_heads=8, dff=512, \n",
    "    vocab_size=1000, max_len=100, num_classes=3\n",
    ")\n",
    "\n",
    "sample_input = tf.random.uniform((2, 50), maxval=1000, dtype=tf.int32)\n",
    "output = model(sample_input)\n",
    "print(\"Model input shape:\", sample_input.shape)\n",
    "print(\"Model output shape:\", output.shape)\n",
    "print(\"Sample predictions:\", output.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 Summary\n",
    "\n",
    "### Key Achievements of Transformers:\n",
    "1. **Self-Attention Mechanism**: Efficiently captures global dependencies in sequences\n",
    "2. **Parallel Computation**: Unlike sequential RNNs, enables parallel processing\n",
    "3. **Scalability**: Can be scaled to very large models (BERT, GPT, T5)\n",
    "4. **State-of-the-Art Performance**: Outperforms previous architectures on various NLP tasks\n",
    "\n",
    "### Core Components:\n",
    "- **Multi-Head Attention**: Attends to multiple representation subspaces simultaneously\n",
    "- **Positional Encoding**: Provides sequence order information\n",
    "- **Residual Connections**: Improves gradient flow during training\n",
    "- **Layer Normalization**: Stabilizes the training process\n",
    "- **Feed-Forward Networks**: Applies non-linear transformations\n",
    "\n",
    "### Applications:\n",
    "- Machine Translation\n",
    "- Text Classification and Sentiment Analysis\n",
    "- Question Answering Systems\n",
    "- Text Generation\n",
    "- Named Entity Recognition\n",
    "- Various other NLP tasks\n",
    "\n",
    "**Transformers have become the foundation for modern NLP models and continue to evolve with architectures like BERT, GPT, T5, and their variants. Their ability to handle long-range dependencies and enable parallel computation has made them the preferred architecture for sequence processing tasks.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
