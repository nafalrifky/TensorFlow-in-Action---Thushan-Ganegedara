{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 â€” State-of-the-Art in Deep Learning: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Text Representation and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Text preprocessing\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Transformers are amazing for NLP tasks\",\n",
    "    \"Attention is all you need for sequence processing\"\n",
    "]\n",
    "\n",
    "# Text vectorization\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=10\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(sample_texts)\n",
    "vectorized_texts = vectorize_layer(sample_texts)\n",
    "\n",
    "print(\"Vectorized texts:\")\n",
    "print(vectorized_texts.numpy())\n",
    "print(f\"Vocabulary size: {len(vectorize_layer.get_vocabulary())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"Scaled dot-product attention implementation\"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "    \n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    \n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test attention\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "depth = 64\n",
    "\n",
    "query = tf.random.normal((batch_size, seq_length, depth))\n",
    "key = tf.random.normal((batch_size, seq_length, depth))\n",
    "value = tf.random.normal((batch_size, seq_length, depth))\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "print(f\"Input shapes - Query: {query.shape}, Key: {key.shape}, Value: {value.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, query, key, value, mask=None):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        query = self.wq(query)\n",
    "        key = self.wk(key)\n",
    "        value = self.wv(value)\n",
    "        \n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            query, key, value, mask\n",
    "        )\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "mha = MultiHeadAttention(d_model=128, num_heads=8)\n",
    "query = tf.random.normal((batch_size, seq_length, 128))\n",
    "output, weights = mha(query, query, query)\n",
    "\n",
    "print(f\"Multi-head attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask=None):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "# Test encoder layer\n",
    "encoder_layer = TransformerEncoderLayer(d_model=128, num_heads=8, dff=512)\n",
    "sample_input = tf.random.normal((batch_size, seq_length, 128))\n",
    "output = encoder_layer(sample_input, training=False)\n",
    "\n",
    "print(f\"Encoder input shape: {sample_input.shape}\")\n",
    "print(f\"Encoder output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"Generate positional encoding\"\"\"\n",
    "    def get_angles(pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "    \n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "    \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Test positional encoding\n",
    "pos_encoding = positional_encoding(50, 128)\n",
    "print(f\"Positional encoding shape: {pos_encoding.shape}\")\n",
    "print(f\"Sample values: {pos_encoding[0, 0, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Complete Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "                 vocab_size, max_position_encoding, num_classes, rate=0.1):\n",
    "        super(TextClassificationTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_position_encoding, d_model)\n",
    "        \n",
    "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate) \n",
    "                         for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.global_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training)\n",
    "        \n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create and test the model\n",
    "transformer = TextClassificationTransformer(\n",
    "    num_layers=2,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    dff=512,\n",
    "    vocab_size=1000,\n",
    "    max_position_encoding=100,\n",
    "    num_classes=3\n",
    ")\n",
    "\n",
    "sample_input = tf.random.uniform((2, 50), maxval=1000, dtype=tf.int32)\n",
    "output = transformer(sample_input, training=False)\n",
    "\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Sample predictions: {output.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Causal Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(5)\n",
    "print(\"Look-ahead mask:\")\n",
    "print(look_ahead_mask.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "lr_schedule = TransformerLearningRateSchedule(128)\n",
    "print(\"Learning rates:\")\n",
    "for step in [1, 1000, 4000]:\n",
    "    print(f\"Step {step}: {lr_schedule(step):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 Summary\n",
    "\n",
    "Key concepts covered:\n",
    "- Self-attention mechanism\n",
    "- Multi-head attention\n",
    "- Positional encoding\n",
    "- Transformer encoder architecture\n",
    "- Practical text classification implementation\n",
    "\n",
    "Transformers have revolutionized NLP with their ability to handle long-range dependencies and parallel computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
