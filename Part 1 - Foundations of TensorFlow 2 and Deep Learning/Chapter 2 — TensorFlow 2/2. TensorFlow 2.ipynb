{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2 — TensorFlow 2\n",
        "Detailed, practical overview of TensorFlow 2: execution model, building blocks, and common neural-network operations. Figures included inline."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 First steps with TensorFlow 2\n",
        "\n",
        "This chapter demonstrates how TensorFlow 2 is designed for both **easy development** (eager execution) and **high-performance production** (graph tracing / compilation). We'll start with a simple Multilayer Perceptron (MLP) example to ground the discussion, then expand into the core concepts that make TensorFlow work under the hood."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.1 A simple MLP (concept)\n",
        "An MLP (fully connected network) has input, hidden, and output layers. For a single hidden layer:\n",
        "- Hidden activation:  $h = \\sigma(x W_1 + b_1)$\n",
        "- Output:  $y = \\mathrm{softmax}(h W_2 + b_2)$\n",
        "\n",
        "Here $\\sigma$ is a nonlinear activation (sigmoid, ReLU, etc.). The MLP illustrates the common workflow: data → linear transform → nonlinearity → output normalization.\n"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.1 — MLP diagram\n",
        "<p align='left'><img src=\"./figure/figure2.1.png\" width=\"60%\"></p>\n",
        "_Figure 2.1 visualizes inputs, weights, biases, hidden units and softmax outputs._"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1.2 Eager execution vs. graph tracing\n",
        "TensorFlow 2 runs in **eager mode** by default — operations execute immediately and return concrete values, making debugging and iteration simple. But for performance, TensorFlow provides `@tf.function` to trace a Python function and compile it into an optimized data-flow graph. The lifecycle looks like:\n",
        "1. **First call**: trace the Python function and build a graph (this can be expensive).\n",
        "2. **Optimize & place ops** on devices (CPU/GPU/TPU).\n",
        "3. **Subsequent calls**: reuse the compiled graph, gaining significant speed.\n",
        "\n",
        "Use `@tf.function` when your function runs many iterations or heavy numeric work; avoid tracing for one-off light operations."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `@tf.function` and AutoGraph (example)\n",
        "AutoGraph converts Python control flow into graph constructs. Here's a minimal example:\n",
        "```python\nimport tensorflow as tf\n\n@tf.function\ndef forward(x, W, b):\n    return tf.matmul(x, W) + b\n\n# tracing happens on first call\nx = tf.constant([[1.0, 2.0]])\nW = tf.constant([[0.5],[0.2]])\nb = tf.constant([0.1])\nprint(forward(x, W, b))\n```\n",
        "**Caveats:** AutoGraph will convert Python lists/NumPy arrays to `tf.constant` and may unroll loops—be careful with very large or variable Python-side structures."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.5 — Tracing + Execution flow\n",
        "<p align='left'><img src=\"./figure/figure2.5.png\" width=\"60%\"></p>\n",
        "_Figure 2.5 illustrates tracing on the first call and feeding inputs on subsequent calls._"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 TensorFlow building blocks\n",
        "Everything in TensorFlow is constructed from three primitives: `tf.Variable`, `tf.Tensor`, and `tf.Operation`. Understanding these is essential even when you use higher-level APIs such as Keras."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### tf.Variable (mutable state)\n",
        "- `tf.Variable` holds parameters that change during training (weights and biases).\n",
        "- Variables expose assign/update methods and interact with optimizers.\n",
        "\n",
        "Example:\n",
        "```python\nimport tensorflow as tf\nW = tf.Variable(tf.random.normal([4, 3]))  # learnable weights\nb = tf.Variable(tf.zeros([3]))\n```"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### tf.Tensor (immutable values)\n",
        "- `tf.Tensor` represents concrete (read-only) data produced by operations.\n",
        "- Use tensors for model inputs, intermediate activations, and constants.\n",
        "\n",
        "Example:\n",
        "```python\nx = tf.constant([[1.0, 2.0, 3.0, 4.0]])\n```"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### tf.Operation (computations)\n",
        "- TensorFlow ops perform transformations: `tf.matmul`, `tf.nn.conv2d`, `tf.add`.\n",
        "- Composing ops yields computation graphs that can be optimized and executed efficiently on accelerators.\n"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.4 — Example computational graph\n",
        "<p align='left'><img src=\"./figure/figure2.4.png\" width=\"60%\"></p>\n",
        "_This figure shows nodes (ops) and tensors flowing between them; higher-level Keras layers are built on these primitives._"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Neural network–related computations\n",
        "This section describes the key ops used in neural networks: matrix multiplication, convolution, and pooling. These are heavily optimized and form the core of NN performance."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3.1 Matrix multiplication (`tf.matmul`)\n",
        "- Dense layers are implemented with matrix multiply followed by bias addition and activation.\n",
        "- TensorFlow calls optimized BLAS/cuBLAS kernels on CPU/GPU for best throughput.\n",
        "\n",
        "Snippet:\n",
        "```python\nx = tf.random.normal([batch_size, input_dim])\nW = tf.Variable(tf.random.normal([input_dim, output_dim]))\nb = tf.Variable(tf.zeros([output_dim]))\nout = tf.matmul(x, W) + b\n```"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3.2 Convolutions (`tf.nn.conv2d` / `tf.nn.convolution`)\n",
        "Convolution is central to CNNs. Key parameters:\n",
        "- Input shape: `[batch, height, width, channels]`\n",
        "- Kernel shape: `[k_h, k_w, in_ch, out_ch]`\n",
        "- Strides and padding (`'SAME'` vs `'VALID'`).\n",
        "\n",
        "Example:\n",
        "```python\nx = tf.random.normal([1, 28, 28, 3])\nkernel = tf.random.normal([5, 5, 3, 32])\nconv = tf.nn.conv2d(x, kernel, strides=[1,1,1,1], padding='SAME')\n```",
        "\n",
        "The chapter includes exercises that show how to reshape inputs and compute output shapes manually."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.2 — Convolution geometry\n",
        "<p align='left'><img src=\"./figure/figure2.2.png\" width=\"60%\"></p>\n",
        "_Shows kernel sliding over input, stride effect and padding choices._"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3.3 Pooling operations (`tf.nn.max_pool2d`, `tf.nn.avg_pool2d`)\n",
        "- Pooling down-samples spatial dimensions and helps make features invariant to small translations.\n",
        "Example:\n",
        "```python\npooled = tf.nn.max_pool2d(conv, ksize=2, strides=2, padding='VALID')\n```"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.3 — Pooling result comparison\n",
        "<p align='left'><img src=\"./figure/figure2.3.png\" width=\"60%\"></p>\n",
        "_Visual comparison of original feature map, average pooling and max pooling outputs._"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Practical considerations and AutoGraph caveats\n",
        "AutoGraph helps convert Python control flow into graph ops, but be mindful:\n",
        "- **Tracing overhead:** functions traced only once; tracing can be expensive if the function is run rarely.\n",
        "- **Implicit conversions:** Python lists/NumPy arrays inside traced functions convert to `tf.constant`.\n",
        "- **Large unrolled loops:** can create enormous graphs—avoid unrolling very large loops inside traced functions.\n",
        "\n",
        "Best practice: keep traced functions focused on numeric ops; move data preparation or Python-only logic outside the traced function. Use `tf.autograph.to_graph` docs for advanced scenarios."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.6 — AutoGraph example (control flow conversion)\n",
        "<p align='left'><img src=\"./figure/figure2.6.png\" width=\"60%\"></p>\n",
        "_Illustrates how an `if`/`for` in Python maps to graph constructs like `tf.cond` and `tf.while_loop`._"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Additional useful utilities\n",
        "TensorFlow provides many helper libraries and tools referenced in this chapter:\n",
        "- **TensorFlow Datasets (tfds)** — ready-made datasets & preprocessing pipelines.\n",
        "- **TensorBoard** — visualize training metrics, computational graphs and profiling information.\n",
        "- **TensorFlow Hub** — repository of reusable pretrained modules.\n",
        "- **Estimator API** — opinionated API useful in robust production workflows (less flexible than Keras, but safer).\n",
        "\n",
        "Figure 2.7 and 2.8 show example TensorBoard views and TF Hub usage diagrams."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.7 — TensorBoard example\n",
        "<p align='left'><img src=\"./figure/figure2.7.png\" width=\"60%\"></p>\n",
        "### Figure 2.8 — TensorFlow Hub concept\n",
        "<p align='left'><img src=\"./figure/figure2.8.png\" width=\"60%\"></p>"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Exercises & quick snippets\n",
        "A few short exercises or code snippets to solidify understanding are included in the book. Two quick runnable examples below:\n",
        "\n",
        "1) **Compute an MLP forward pass (eager)**\n",
        "```python\nimport tensorflow as tf\nx = tf.constant([[1.,2.,3.,4.]])\nW1 = tf.random.normal([4,3])\nb1 = tf.zeros([3])\nh = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\nW2 = tf.random.normal([3,2])\nb2 = tf.zeros([2])\ny = tf.nn.softmax(tf.matmul(h, W2) + b2)\nprint(y)\n```\n",
        "2) **Wrap forward in @tf.function**\n",
        "```python\n@tf.function\ndef forward(x, W1, b1, W2, b2):\n    h = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n    return tf.nn.softmax(tf.matmul(h, W2) + b2)\n```\n",
        "\n",
        "Run these in a live kernel to see eager vs compiled behavior."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figures: remaining visuals\n",
        "<p align='left'><img src=\"./figure/figure2.9.png\" width=\"48%\"> <img src=\"./figure/figure2.10.png\" width=\"48%\"></p>\n",
        "<p align='left'><img src=\"./figure/figure2.11.png\" width=\"48%\"> <img src=\"./figure/figure2.12.png\" width=\"48%\"></p>\n",
        "<p align='left'><img src=\"./figure/figure2.13.png\" width=\"60%\"></p>\n",
        "_These figures correspond to additional diagrams and sample outputs found in Chapter 2 (kernel execution, profiling view, convolution examples, etc.)._"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 2 — Summary (key takeaways)\n",
        "- TensorFlow 2 favors eager execution for developer ergonomics while enabling graph compilation via `@tf.function` for performance.\n",
        "- Core primitives: `tf.Variable` (mutable state), `tf.Tensor` (immutable arrays), `tf.Operation` (computational ops).\n",
        "- Common NN ops (matmul, conv, pooling) are optimized on CPU/GPU/TPU.\n",
        "- AutoGraph reduces boilerplate but requires careful use to avoid tracing overhead or big graphs.\n",
        "\n",
        "If you want, I can now:\n",
        "1. Save this as a `.ipynb` file in `/mnt/data/Chapter2_TensorFlow2.ipynb` for direct download, or\n",
        "2. Add more runnable cells and unit tests, or\n",
        "3. Convert everything into a markdown file for README.\n",
        "\n",
        "Which of the above would you like next?"
      ]
    }
  ]
}

